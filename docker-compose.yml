services:
  # PostgreSQL database for production
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: pirate_on_rails_production
      POSTGRES_USER: pirate_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-your_secure_password_here}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    command: >
      postgres 
      -c shared_preload_libraries=pg_stat_statements
      -c pg_stat_statements.track=all
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U pirate_user -d pirate_on_rails_production"]
      interval: 30s
      timeout: 10s
      retries: 3

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - bootstrap.memory_lock=true
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: >
      redis-server 
      --appendonly yes 
      --appendfsync everysec
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 60
      --timeout 300
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Rails web application
  web:
    build:
      context: .
      dockerfile: Dockerfile
    expose:
      - "3000"  # Changed from ports to expose since Nginx will proxy
    environment:
      - RAILS_ENV=production
      - POSTGRES_HOST=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-your_secure_password_here}
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./storage:/rails/storage
      - ./log:/rails/log
      - static_files:/rails/public  # Share static files with Nginx
    restart: unless-stopped
    env_file:
      - .env

  # Nginx reverse proxy and static file server
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"  # For future SSL support
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - static_files:/var/www/public:ro  # Access Rails static files
      - ./log:/var/log/nginx  # Share log directory
    depends_on:
      - web
      - python-ai
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Python AI Microservice
  python-ai:
    build:
      context: ./python-ai-service
      dockerfile: Dockerfile
    expose:
      - "8000"  # Changed from ports to expose since Nginx will proxy
    environment:
      - SERIES_DATA_URL=http://web:3000/chats/series_data
      - RAILS_HISTORY_URL=http://web:3000/chats/history
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    depends_on:
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    volumes:
      - ai_models:/app/models  # For caching ML models
      - ai_data:/app/data      # For FAISS indices
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  postgres_data:
    driver: local
  esdata:
    driver: local
  redis_data:
    driver: local
  ai_models:
    driver: local
  ai_data:
    driver: local
  static_files:
    driver: local